"""
Transcription API endpoints
Provides model information and estimated transcription times
"""
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from loguru import logger


class WhisperModel(BaseModel):
    """Whisper model info"""
    id: str
    name: str
    backend: str  # "faster" or "openai"
    size_mb: int
    vram_mb: int
    quality: str  # "basic", "good", "better", "great", "best"
    speed_factor_cpu: float  # relative to realtime on CPU (e.g., 15 = 15x realtime)
    speed_factor_gpu: float  # relative to realtime on GPU
    recommended_for: List[str]
    description: str


class TranscriptionEstimate(BaseModel):
    """Estimated transcription time"""
    model_id: str
    duration_seconds: float
    estimated_time_cpu_seconds: float
    estimated_time_gpu_seconds: float
    estimated_time_cpu_formatted: str
    estimated_time_gpu_formatted: str


# Model definitions with performance data
WHISPER_MODELS: List[Dict[str, Any]] = [
    # Faster-Whisper models (CTranslate2 backend) - 4-8x faster than OpenAI
    {
        "id": "faster:tiny",
        "name": "âš¡ Faster-Whisper Tiny (æžé€Ÿ)",
        "backend": "faster",
        "size_mb": 75,
        "vram_mb": 500,
        "quality": "basic",
        "speed_factor_cpu": 20.0,  # 20x realtime on CPU
        "speed_factor_gpu": 50.0,
        "recommended_for": ["å¿«é€Ÿæµ‹è¯•", "ç®€å•å¯¹è¯"],
        "description": "Faster-Whisper: æœ€å¿«é€Ÿåº¦ï¼Œé€‚åˆæµ‹è¯•ã€‚å‡†ç¡®çŽ‡è¾ƒä½Žã€‚",
    },
    {
        "id": "faster:base",
        "name": "âš¡ Faster-Whisper Base (å¿«é€Ÿ)",
        "backend": "faster",
        "size_mb": 150,
        "vram_mb": 700,
        "quality": "good",
        "speed_factor_cpu": 15.0,  # 15x realtime on CPU
        "speed_factor_gpu": 40.0,
        "recommended_for": ["æ—¥å¸¸è§†é¢‘", "æ¸…æ™°è¯­éŸ³"],
        "description": "Faster-Whisper: é€Ÿåº¦å¿«ï¼Œå‡†ç¡®çŽ‡é€‚ä¸­ã€‚æŽ¨èæ—¥å¸¸ä½¿ç”¨ã€‚",
    },
    {
        "id": "faster:small",
        "name": "âš¡ Faster-Whisper Small (å‡è¡¡) â­æŽ¨è",
        "backend": "faster",
        "size_mb": 500,
        "vram_mb": 1500,
        "quality": "better",
        "speed_factor_cpu": 8.0,  # 8x realtime on CPU
        "speed_factor_gpu": 25.0,
        "recommended_for": ["å¤§éƒ¨åˆ†è§†é¢‘", "è½»åº¦å£éŸ³"],
        "description": "Faster-Whisper: é€Ÿåº¦ä¸Žè´¨é‡å‡è¡¡ã€‚æŽ¨èä½œä¸ºé»˜è®¤é€‰æ‹©ã€‚",
    },
    {
        "id": "faster:medium",
        "name": "âš¡ Faster-Whisper Medium (é«˜è´¨é‡)",
        "backend": "faster",
        "size_mb": 1500,
        "vram_mb": 3000,
        "quality": "great",
        "speed_factor_cpu": 3.0,  # 3x realtime on CPU
        "speed_factor_gpu": 15.0,
        "recommended_for": ["ä¸“ä¸šå†…å®¹", "å¤šè¯­è¨€æ··åˆ"],
        "description": "Faster-Whisper: é«˜å‡†ç¡®çŽ‡ï¼Œé€‚åˆä¸“ä¸šå†…å®¹ã€‚é€Ÿåº¦ä¸­ç­‰ã€‚",
    },
    {
        "id": "faster:large-v3",
        "name": "âš¡ Faster-Whisper Large-V3 (æœ€ä½³)",
        "backend": "faster",
        "size_mb": 3000,
        "vram_mb": 6000,
        "quality": "best",
        "speed_factor_cpu": 0.8,  # 0.8x realtime on CPU (slower than realtime)
        "speed_factor_gpu": 8.0,
        "recommended_for": ["å›°éš¾éŸ³é¢‘", "å¼ºå£éŸ³", "èƒŒæ™¯å™ªéŸ³"],
        "description": "Faster-Whisper: æœ€é«˜å‡†ç¡®çŽ‡ï¼ŒCPUè¾ƒæ…¢ã€‚é€‚åˆå›°éš¾éŸ³é¢‘ã€‚",
    },
    # WhisperX models (word-level alignment, best for subtitles)
    {
        "id": "whisperx:tiny",
        "name": "ðŸŽ¯ WhisperX Tiny (æžé€Ÿ)",
        "backend": "whisperx",
        "size_mb": 75,
        "vram_mb": 600,
        "quality": "basic",
        "speed_factor_cpu": 15.0,
        "speed_factor_gpu": 40.0,
        "recommended_for": ["å¿«é€Ÿæµ‹è¯•", "è¯çº§å¯¹é½"],
        "description": "WhisperX: æžé€Ÿ+è¯çº§å¯¹é½ï¼Œé€‚åˆæµ‹è¯•ã€‚",
    },
    {
        "id": "whisperx:base",
        "name": "ðŸŽ¯ WhisperX Base (å¿«é€Ÿ)",
        "backend": "whisperx",
        "size_mb": 150,
        "vram_mb": 800,
        "quality": "good",
        "speed_factor_cpu": 12.0,
        "speed_factor_gpu": 35.0,
        "recommended_for": ["æ—¥å¸¸è§†é¢‘", "è¯çº§å¯¹é½"],
        "description": "WhisperX: å¿«é€Ÿ+è¯çº§å¯¹é½ï¼Œæ—¥å¸¸ä½¿ç”¨ã€‚",
    },
    {
        "id": "whisperx:small",
        "name": "ðŸŽ¯ WhisperX Small (å‡è¡¡)",
        "backend": "whisperx",
        "size_mb": 500,
        "vram_mb": 2000,
        "quality": "better",
        "speed_factor_cpu": 6.0,
        "speed_factor_gpu": 20.0,
        "recommended_for": ["ç²¾ç¡®å­—å¹•", "åˆ†æ®µä¼˜åŒ–"],
        "description": "WhisperX: å‡è¡¡+è¯çº§å¯¹é½ï¼ŒæŽ¨èæ—¥å¸¸ä½¿ç”¨ã€‚",
    },
    {
        "id": "whisperx:medium",
        "name": "ðŸŽ¯ WhisperX Medium (é«˜è´¨é‡)",
        "backend": "whisperx",
        "size_mb": 1500,
        "vram_mb": 4000,
        "quality": "great",
        "speed_factor_cpu": 2.5,
        "speed_factor_gpu": 12.0,
        "recommended_for": ["ç²¾ç¡®å­—å¹•", "å¤šè¯­è¨€"],
        "description": "WhisperX: é«˜è´¨é‡+è¯çº§å¯¹é½ï¼Œä¸“ä¸šå†…å®¹ã€‚",
    },
    {
        "id": "whisperx:large-v3",
        "name": "ðŸŽ¯ WhisperX Large-V3 (æœ€ä½³) â­æŽ¨è",
        "backend": "whisperx",
        "size_mb": 3000,
        "vram_mb": 7000,
        "quality": "best",
        "speed_factor_cpu": 0.6,
        "speed_factor_gpu": 6.0,
        "recommended_for": ["æœ€ä½³å­—å¹•", "å›°éš¾éŸ³é¢‘"],
        "description": "WhisperX: æœ€é«˜è´¨é‡+è¯çº§å¯¹é½ï¼Œå­—å¹•æ•ˆæžœæœ€ä½³ã€‚",
    },
    # OpenAI Whisper models (standard, slower but more compatible)
    {
        "id": "openai:tiny",
        "name": "ðŸ¢ OpenAI Whisper Tiny",
        "backend": "openai",
        "size_mb": 75,
        "vram_mb": 500,
        "quality": "basic",
        "speed_factor_cpu": 8.0,
        "speed_factor_gpu": 30.0,
        "recommended_for": ["æµ‹è¯•", "MPS/GPUåŠ é€Ÿ"],
        "description": "åŽŸç‰ˆWhisper: æ”¯æŒMPS(Mac GPU)ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚",
    },
    {
        "id": "openai:base",
        "name": "ðŸ¢ OpenAI Whisper Base",
        "backend": "openai",
        "size_mb": 150,
        "vram_mb": 700,
        "quality": "good",
        "speed_factor_cpu": 5.0,
        "speed_factor_gpu": 20.0,
        "recommended_for": ["é€šç”¨", "MPS/GPUåŠ é€Ÿ"],
        "description": "åŽŸç‰ˆWhisper: æ”¯æŒMPS(Mac GPU)ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚",
    },
    {
        "id": "openai:small",
        "name": "ðŸ¢ OpenAI Whisper Small",
        "backend": "openai",
        "size_mb": 500,
        "vram_mb": 1500,
        "quality": "better",
        "speed_factor_cpu": 2.5,
        "speed_factor_gpu": 12.0,
        "recommended_for": ["é€šç”¨", "MPS/GPUåŠ é€Ÿ"],
        "description": "åŽŸç‰ˆWhisper: æ”¯æŒMPS(Mac GPU)ï¼ŒCPUè¾ƒæ…¢ã€‚",
    },
    {
        "id": "openai:medium",
        "name": "ðŸ¢ OpenAI Whisper Medium",
        "backend": "openai",
        "size_mb": 1500,
        "vram_mb": 3000,
        "quality": "great",
        "speed_factor_cpu": 1.0,
        "speed_factor_gpu": 6.0,
        "recommended_for": ["é«˜è´¨é‡éœ€æ±‚", "MPS/GPUåŠ é€Ÿ"],
        "description": "åŽŸç‰ˆWhisper: æ”¯æŒMPS(Mac GPU)ï¼ŒCPUå¾ˆæ…¢ã€‚",
    },
    {
        "id": "openai:large-v3",
        "name": "ðŸ¢ OpenAI Whisper Large-V3",
        "backend": "openai",
        "size_mb": 3000,
        "vram_mb": 6000,
        "quality": "best",
        "speed_factor_cpu": 0.3,  # Very slow on CPU
        "speed_factor_gpu": 4.0,
        "recommended_for": ["æœ€é«˜è´¨é‡", "MPS/GPUåŠ é€Ÿ"],
        "description": "åŽŸç‰ˆWhisper: æ”¯æŒMPS(Mac GPU)ï¼ŒCPUéžå¸¸æ…¢ã€‚",
    },
]


def get_whisper_models(backend: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Get available Whisper models.

    Args:
        backend: Filter by backend ("faster" or "openai"), None for all

    Returns:
        List of model info dicts
    """
    if backend:
        return [m for m in WHISPER_MODELS if m["backend"] == backend]
    return WHISPER_MODELS


def get_model_by_id(model_id: str) -> Optional[Dict[str, Any]]:
    """Get model info by ID"""
    for model in WHISPER_MODELS:
        if model["id"] == model_id:
            return model
    return None


def format_time(seconds: float) -> str:
    """Format seconds as human readable time"""
    if seconds < 60:
        return f"{int(seconds)}ç§’"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes}åˆ†{secs}ç§’"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        return f"{hours}å°æ—¶{minutes}åˆ†"


def estimate_transcription_time(
    duration_seconds: float,
    model_id: str = "faster:base",
    device: str = "cpu"
) -> Dict[str, Any]:
    """
    Estimate transcription time for a video.

    Args:
        duration_seconds: Video duration in seconds
        model_id: Model ID (e.g., "faster:base")
        device: "cpu" or "gpu"

    Returns:
        Estimation dict with times
    """
    model = get_model_by_id(model_id)
    if not model:
        # Default to faster:base
        model = get_model_by_id("faster:base")

    speed_cpu = model["speed_factor_cpu"]
    speed_gpu = model["speed_factor_gpu"]

    est_cpu = duration_seconds / speed_cpu
    est_gpu = duration_seconds / speed_gpu

    return {
        "model_id": model_id,
        "model_name": model["name"],
        "duration_seconds": duration_seconds,
        "duration_formatted": format_time(duration_seconds),
        "estimated_time_cpu_seconds": est_cpu,
        "estimated_time_gpu_seconds": est_gpu,
        "estimated_time_cpu_formatted": format_time(est_cpu),
        "estimated_time_gpu_formatted": format_time(est_gpu),
        "device": device,
        "estimated_time_seconds": est_cpu if device == "cpu" else est_gpu,
        "estimated_time_formatted": format_time(est_cpu) if device == "cpu" else format_time(est_gpu),
    }


def get_recommended_model(duration_seconds: float, device: str = "cpu") -> str:
    """
    Get recommended model based on video duration and device.

    Args:
        duration_seconds: Video duration
        device: "cpu" or "gpu"

    Returns:
        Recommended model ID
    """
    if device == "gpu":
        # GPU can handle larger models efficiently
        if duration_seconds > 3600:  # > 1 hour
            return "faster:medium"
        else:
            return "faster:large-v3"
    else:
        # CPU - balance speed and quality
        if duration_seconds > 3600:  # > 1 hour
            return "faster:base"
        elif duration_seconds > 1800:  # > 30 min
            return "faster:small"
        elif duration_seconds > 600:  # > 10 min
            return "faster:small"
        else:
            return "faster:medium"


def get_all_estimates(duration_seconds: float, backend: Optional[str] = None) -> List[Dict[str, Any]]:
    """Get estimates for all models"""
    estimates = []
    for model in WHISPER_MODELS:
        # Filter by backend if specified
        if backend and model["backend"] != backend:
            continue
        est = estimate_transcription_time(duration_seconds, model["id"], "cpu")
        est["quality"] = model["quality"]
        est["description"] = model["description"]
        est["recommended_for"] = model["recommended_for"]
        est["backend"] = model["backend"]
        estimates.append(est)
    return estimates
